{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from corvus_python.pyspark.storage import LocalFileSystemStorageConfiguration\n",
    "from corvus_python.pyspark.utilities import create_spark_session\n",
    "\n",
    "file_system_configuration = LocalFileSystemStorageConfiguration(\"sample_entrypoints/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/workspaces/Corvus.Python/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "com.microsoft.sqlserver#mssql-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3dcd9040-cb35-4695-b836-12349902bb6e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;12.6.0.jre11 in central\n",
      ":: resolution report :: resolve 259ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;12.6.0.jre11 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3dcd9040-cb35-4695-b836-12349902bb6e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/12ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/15 13:20:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session(\"test_notebook\", file_system_configuration, enable_hive_support=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Corvus.Python/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/workspaces/Corvus.Python/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|   Name|Metric1|Metric2|Metric3|\n",
      "+-------+-------+-------+-------+\n",
      "|  Alice|     10|     40|     70|\n",
      "|    Bob|     20|     50|     80|\n",
      "|Charlie|     30|     60|     90|\n",
      "+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Metric1': [10, 20, 30],\n",
    "    'Metric2': [40, 50, 60],\n",
    "    'Metric3': [70, 80, 90]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Metric10', Metric1\", \"'Metric2', Metric2\", \"'Metric30', Metric3\"]\n",
      "stack(3, 'Metric10', Metric1, 'Metric2', Metric2, 'Metric30', Metric3) as (variable, value)\n",
      "+-------+--------+-----+\n",
      "|   Name|variable|value|\n",
      "+-------+--------+-----+\n",
      "|  Alice|Metric10|   10|\n",
      "|  Alice| Metric2|   40|\n",
      "|  Alice|Metric30|   70|\n",
      "|    Bob|Metric10|   20|\n",
      "|    Bob| Metric2|   50|\n",
      "|    Bob|Metric30|   80|\n",
      "|Charlie|Metric10|   30|\n",
      "|Charlie| Metric2|   60|\n",
      "|Charlie|Metric30|   90|\n",
      "+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class RenameMapping:\n",
    "    original_name: str\n",
    "    new_name: str\n",
    "\n",
    "\n",
    "# Replace with pyspark.sql.DataFrame.unpivot when using version 3.4.0. Function signature is the same.\n",
    "def spark_unpivot(df, ids, values=None, variableColumnName='variable', valueColumnName='value'):\n",
    "    if not values:\n",
    "        values = [c for c in df.columns if c not in ids]\n",
    "    \n",
    "    values_expr = ', '.join([f\"'{c}', {c}\" for c in values])\n",
    "\n",
    "    stack_select_expr = f\"stack({len(values)}, {values_expr}) as ({variableColumnName}, {valueColumnName})\"\n",
    "    print(stack_select_expr)\n",
    "\n",
    "    return df.selectExpr(*ids, stack_select_expr)\n",
    "\n",
    "def spark_unpivot_with_rename(\n",
    "        df: DataFrame,\n",
    "        ids: List[str],\n",
    "        values: List[str] = None,\n",
    "        variableColumnName: str = 'variable',\n",
    "        valueColumnName: str = 'value',\n",
    "        renameMappings: List[RenameMapping] = None\n",
    "        ):\n",
    "    if not values:\n",
    "        values = [c for c in df.columns if c not in ids]\n",
    "    \n",
    "    if renameMappings:\n",
    "        value_pairs = []\n",
    "        for v in values:\n",
    "            rm = next((rm for rm in renameMappings if rm.original_name == v), None)\n",
    "            if rm:\n",
    "                value_pair = f\"'{rm.new_name}', {rm.original_name}\"\n",
    "            else:\n",
    "                value_pair = f\"'{v}', {v}\"\n",
    "\n",
    "            value_pairs.append(value_pair)\n",
    "        \n",
    "        values_expr = ', '.join(value_pairs)\n",
    "    else:\n",
    "        values_expr = ', '.join([f\"'{c}', {c}\" for c in values])\n",
    "\n",
    "    stack_select_expr = f\"stack({len(values)}, {values_expr}) as ({variableColumnName}, {valueColumnName})\"\n",
    "\n",
    "    return df.selectExpr(*ids, stack_select_expr)\n",
    "\n",
    "df_new = spark_unpivot_with_rename(\n",
    "    df,\n",
    "    ['Name'],\n",
    "    ['Metric1', 'Metric2', 'Metric3'],\n",
    "    renameMappings=[\n",
    "        RenameMapping(original_name='Metric1', new_name='Metric10'),\n",
    "        # RenameMapping(original_name='Metric2', new_name='Metric20'),\n",
    "        RenameMapping(original_name='Metric3', new_name='Metric30')\n",
    "        ]\n",
    ")\n",
    "# df = spark_unpivot(df, ['Name'], ['Metric1', 'Metric2', 'Metric3'])\n",
    "\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
